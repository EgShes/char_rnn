{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Посимвольная языковая модель.\n",
    "\n",
    "В первом задании Вам нужно написать и обучить посимвольную нейронную языковую модель для вычисления вероятностей буквенных последовательностей (то есть слов). Такие модели используются в задачах словоизменения и распознавания/порождения звучащей речи. Для обучения модели используйте данные для русского языка из [репозитория](https://github.com/sigmorphon/conll2018/tree/master/task1/surprise).\n",
    "\n",
    "**В процессе написания Вам нужно решить следующие проблемы:**\n",
    "    \n",
    "* как будет выглядеть обучающая выборка; что будет являться признаками, и что - метками классов.\n",
    "* как сделать так, чтобы модель при предсказании символа учитывала все предыдущие символы слова.\n",
    "* какие специальные символы нужно использовать.\n",
    "* как передавать в модель текущее состояние рекуррентной сети\n",
    "\n",
    "**Результаты:**\n",
    "\n",
    "* предобработчик данных,\n",
    "* генератор обучающих данных (батчей),\n",
    "* обученная модель\n",
    "* перплексия модели на настроечной выборке\n",
    "* посимвольные вероятности слов в контрольной выборке\n",
    "\n",
    "**Дополнительно:**\n",
    "\n",
    "* дополнительный вход модели (часть речи слова, другие морфологические признаки), влияет ли его добавление на перплексию\n",
    "* сравнение различных архитектур нейронной сети (FC, RNN, LSTM, QRNN, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it is better to do all imports at the first cell\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.nn import Embedding, RNN, GRU, LSTM, Linear\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from re import findall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to download data\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-train-high\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-dev\n",
    "# !wget https://github.com/sigmorphon/conll2018/blob/master/task1/surprise/russian-covered-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(infile):\n",
    "    words, tags = [], []\n",
    "    with open(infile, \"r\", encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            splitted = line.split(\"\\t\")\n",
    "            if len(splitted) == 3:\n",
    "                words.append(findall(r'[а-яё]+', splitted[0].lower())[0])\n",
    "                tags.append(splitted[2].split('<')[0].split(';')[:])\n",
    "            if len(splitted) == 2:\n",
    "                words.append(findall(r'[а-яё]+', splitted[0].lower())[0])\n",
    "                tags.append(splitted[1].split('<')[0].split(';')[:])\n",
    "    return words, tags\n",
    "\n",
    "train_words, train_tags = read_dataset(\"russian-train-high\")\n",
    "dev_words, dev_tags = read_dataset(\"russian-dev\")\n",
    "test_words, test_tags = read_dataset(\"russian-covered-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examination of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000, 1000, 1000, 1000, 1000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words), len(train_tags), len(dev_words), len(dev_tags), len(test_words), len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: насылать ; tags: ['ADJ', 'DAT', 'NEUT', 'SG']\n",
      "word: разостлать ; tags: ['ADJ', 'INS', 'NEUT', 'SG']\n",
      "word: жэк ; tags: ['V.CVB', 'PST']\n",
      "word: гастрольный ; tags: ['ADJ', 'ANIM', 'ACC', 'MASC', 'SG']\n",
      "word: литьё ; tags: ['N', 'DAT', 'PL']\n",
      "word: иноплеменный ; tags: ['ADJ', 'INS', 'NEUT', 'SG']\n",
      "word: блестеть ; tags: ['N', 'NOM', 'PL']\n",
      "word: копаться ; tags: ['V', 'PST', 'SG', 'FEM']\n",
      "word: подъезд ; tags: ['V', 'FUT', '2', 'PL']\n",
      "word: кувырнуться ; tags: ['ADJ', 'FEM', 'SG', 'LGSPEC1']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('word:', dev_words[i], '; tags:', train_tags[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['валлонский', 'незаконченный', 'истрёпывать', 'личный', 'серьга']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подумайте, какие вспомогательные токены могут быть вам полезны. Выдайте им индексы от `0` до `len(AUXILIARY) - 1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will add tokens BEG and END. They will help model to better understand beginnings and endings. Also I will add UNK token in the case if a symbol was missed in training data and PAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUXILIARY = ['_', '@', '<', '>'] # padding, unknown, beggining, ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def fit(self, data):\n",
    "        \"\"\"Extract unique symbols from the data, make itos (item to string) and stoi (string to index) objects\"\"\"\n",
    "        symbols = set(x for elem in data for x in elem)\n",
    "        self._symbols = AUXILIARY + sorted(symbols)\n",
    "        # Запомните следующую строчку кода - она нужна примерно всегда\n",
    "        self._s2i = {s: i for i, s in enumerate(self._symbols)}\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._symbols)\n",
    "\n",
    "    def transform(self, data):\n",
    "        \"\"\"Transform data to indices\n",
    "        Input:\n",
    "            - data, list of strings\n",
    "        Output:\n",
    "            - list of list of char indices\n",
    "\n",
    "        >>> self.transform(['word1', 'token2'])\n",
    "        >>> [[24, 2, 19, 13, 3], [8, 2, 9, 1, 7, 4]]\n",
    "        \"\"\"\n",
    "        transformed_words = []\n",
    "        for word in data:\n",
    "            tr_word = []\n",
    "            for s in word:\n",
    "                tr_word.append(self._s2i[s] if s in self._s2i else self._s2i['@'])\n",
    "            transformed_words.append([self._s2i['<']] + tr_word + [self._s2i['>']])\n",
    "        return transformed_words\n",
    "    \n",
    "    def s2i(self, symbol):\n",
    "        assert(symbol in self._s2i)\n",
    "        return self._s2i[symbol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 16, 4, 28, 12, 17, 4, 3], [2, 21, 4, 16, 18, 15, 36, 22, 3]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_voc = Vocabulary()\n",
    "char_voc.fit(train_words)\n",
    "char_voc.transform(['машина', 'самолёт'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.char_vocab = Vocabulary()\n",
    "        self.char_vocab.fit(data)\n",
    "        self.proc_data = self.char_vocab.transform(data)\n",
    "        self.proc_data.sort(key = lambda w: len(w), reverse=True)\n",
    "        self.proc_data = self._pad(self.proc_data)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.proc_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.tensor(self.proc_data[index][:-1], dtype=torch.long)\n",
    "        y = torch.tensor(self.proc_data[index][1:], dtype=torch.long)\n",
    "        return x, y\n",
    "    \n",
    "    def _pad(self, data, maxlen=None):\n",
    "        if maxlen is None:\n",
    "            maxlen = max([len(w) for w in data])\n",
    "        for i in range(len(data)):\n",
    "            data[i] = data[i] + [0] * (maxlen - len(data[i]))\n",
    "        return data\n",
    "    \n",
    "    def get_word(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CharDataset(train_words)\n",
    "val_dataset = CharDataset(dev_words)\n",
    "test_dataset = CharDataset(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 37 37\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset.char_vocab), len(val_dataset.char_vocab), len(test_dataset.char_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 24]) torch.Size([5, 24])\n",
      "tensor([[ 2,  6, 31, 21, 18, 14, 18, 14,  6,  4, 15, 12, 24, 12, 26, 12, 20, 18,\n",
      "          6,  4, 17, 17, 31, 13],\n",
      "        [ 2,  7, 12,  8, 20, 18, 16,  9, 22,  9, 18, 20, 18, 15, 18,  7, 12, 27,\n",
      "          9, 21, 14, 12, 13,  3],\n",
      "        [ 2, 20,  4,  8, 12, 18, 14, 18, 16, 16, 23, 17, 12, 14,  4, 26, 12, 18,\n",
      "         17, 17, 31, 13,  3,  0],\n",
      "        [ 2, 14, 18, 17, 22, 20, 22,  9, 20, 20, 18, 20, 12, 21, 22, 12, 27,  9,\n",
      "         21, 14, 12, 13,  3,  0],\n",
      "        [ 2,  6, 31, 21, 18, 14, 18, 25, 23,  8, 18, 10,  9, 21, 22,  6,  9, 17,\n",
      "         17, 31, 13,  3,  0,  0]]) \n",
      " tensor([[ 6, 31, 21, 18, 14, 18, 14,  6,  4, 15, 12, 24, 12, 26, 12, 20, 18,  6,\n",
      "          4, 17, 17, 31, 13,  3],\n",
      "        [ 7, 12,  8, 20, 18, 16,  9, 22,  9, 18, 20, 18, 15, 18,  7, 12, 27,  9,\n",
      "         21, 14, 12, 13,  3,  0],\n",
      "        [20,  4,  8, 12, 18, 14, 18, 16, 16, 23, 17, 12, 14,  4, 26, 12, 18, 17,\n",
      "         17, 31, 13,  3,  0,  0],\n",
      "        [14, 18, 17, 22, 20, 22,  9, 20, 20, 18, 20, 12, 21, 22, 12, 27,  9, 21,\n",
      "         14, 12, 13,  3,  0,  0],\n",
      "        [ 6, 31, 21, 18, 14, 18, 25, 23,  8, 18, 10,  9, 21, 22,  6,  9, 17, 17,\n",
      "         31, 13,  3,  0,  0,  0]])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x,'\\n', y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRnnModel(Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, emb_dim, vocab_len, num_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.embed = Embedding(vocab_len, emb_dim)\n",
    "        self.gru = GRU(emb_dim, hidden_size, num_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = Linear(hidden_size, vocab_len)\n",
    "        \n",
    "    def forward(self, inputs, h0=None):\n",
    "        if len(inputs.shape) == 0:\n",
    "            inputs = inputs.unsqueeze(0).unsqueeze(0)\n",
    "        if len(inputs.shape) == 1:\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "        print(inputs.shape)\n",
    "        emb = self.embed(inputs)\n",
    "        print(emb.shape)\n",
    "        if h0 is None:\n",
    "            gru_out, gru_hidden = self.gru(emb)\n",
    "        else:\n",
    "            gru_out, gru_hidden = self.gru(emb, h0)\n",
    "        print(gru_out.shape, gru_hidden.shape)\n",
    "        out = self.fc(gru_out)\n",
    "        print(out.shape)\n",
    "        return out, gru_hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CharRnnModel(100, 105, len(char_voc), 2)\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 24]) torch.Size([5, 24])\n",
      "torch.Size([5, 24])\n",
      "torch.Size([5, 24, 105])\n",
      "torch.Size([5, 24, 100]) torch.Size([2, 5, 100])\n",
      "torch.Size([5, 24, 37])\n",
      "torch.Size([5, 24, 37]) torch.Size([5, 24])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_loader:\n",
    "    h0 = model.init_hidden(train_loader.batch_size)\n",
    "    print(x.shape, y.shape)\n",
    "    out, h0 = model(x, h0)\n",
    "    print(out.shape, y.shape)\n",
    "    loss = criterion(out.permute(0,2,1), y)\n",
    "#     print(loss)\n",
    "#     print(h0)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.Size([10, 29]) 10\n",
    "torch.Size([2, 10, 100])\n",
    "torch.Size([10]) torch.Size([2, 10, 100])\n",
    "torch.Size([10, 29, 300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6931)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(torch.tensor(2, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    \n",
    "    pred_lbl = F.softmax(pred, dim=2).argmax(dim=2)\n",
    "    correct = (pred_lbl == y).float()\n",
    "    acc = torch.mean(correct.sum(dim=1) / correct.shape[1])\n",
    "    return acc\n",
    "\n",
    "def perplexity(pred):\n",
    "    \n",
    "    pred_probs = torch.max(F.softmax(pred, dim=2), dim=2)[0]\n",
    "    perplexity = torch.exp(-torch.mean(torch.log(pred_probs)))\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_perplexity = 0\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        pred, _ = model(x)\n",
    "        loss = criterion(pred.permute(0,2,1), y)\n",
    "        acc = accuracy(pred, y)\n",
    "        perplx = perplexity(pred)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_perplexity += perplx.item()\n",
    "        \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader), epoch_perplexity / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_perplexity = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            pred, _ = model(x)\n",
    "            loss = criterion(pred.permute(0,2,1), y)\n",
    "            acc = accuracy(pred, y)\n",
    "            perplx = perplexity(pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_perplexity += perplx.item()\n",
    "\n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader), epoch_perplexity / len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 | train: loss 1.3684, acc 0.637, perplexity: 4.0650 | val: loss 1.9079, acc 0.642, perplexity 1.3865\n",
      "2 | train: loss 1.2171, acc 0.688, perplexity: 1.8636 | val: loss 0.9785, acc 0.716, perplexity 1.8028\n",
      "3 | train: loss 0.9749, acc 0.716, perplexity: 1.8008 | val: loss 0.9518, acc 0.721, perplexity 1.6678\n",
      "4 | train: loss 0.9454, acc 0.723, perplexity: 1.7406 | val: loss 0.9182, acc 0.731, perplexity 1.6325\n",
      "5 | train: loss 0.9204, acc 0.728, perplexity: 1.7078 | val: loss 0.8974, acc 0.737, perplexity 1.6208\n",
      "6 | train: loss 0.9022, acc 0.732, perplexity: 1.6857 | val: loss 0.8792, acc 0.741, perplexity 1.6107\n",
      "7 | train: loss 0.8866, acc 0.736, perplexity: 1.6663 | val: loss 0.8652, acc 0.744, perplexity 1.6031\n",
      "8 | train: loss 0.8730, acc 0.739, perplexity: 1.6513 | val: loss 0.8518, acc 0.746, perplexity 1.5943\n",
      "9 | train: loss 0.8605, acc 0.741, perplexity: 1.6391 | val: loss 0.8420, acc 0.749, perplexity 1.5832\n",
      "10 | train: loss 0.8494, acc 0.744, perplexity: 1.6275 | val: loss 0.8329, acc 0.751, perplexity 1.5719\n",
      "11 | train: loss 0.8404, acc 0.746, perplexity: 1.6173 | val: loss 0.8240, acc 0.753, perplexity 1.5623\n",
      "12 | train: loss 0.8313, acc 0.749, perplexity: 1.6088 | val: loss 0.8170, acc 0.754, perplexity 1.5527\n",
      "13 | train: loss 0.8227, acc 0.751, perplexity: 1.5997 | val: loss 0.8111, acc 0.755, perplexity 1.5422\n",
      "14 | train: loss 0.8148, acc 0.753, perplexity: 1.5924 | val: loss 0.8043, acc 0.757, perplexity 1.5370\n",
      "15 | train: loss 0.8063, acc 0.755, perplexity: 1.5838 | val: loss 0.7965, acc 0.759, perplexity 1.5331\n",
      "16 | train: loss 0.7979, acc 0.757, perplexity: 1.5768 | val: loss 0.7908, acc 0.760, perplexity 1.5252\n",
      "17 | train: loss 0.7902, acc 0.760, perplexity: 1.5700 | val: loss 0.7855, acc 0.761, perplexity 1.5177\n",
      "18 | train: loss 0.7836, acc 0.761, perplexity: 1.5629 | val: loss 0.7794, acc 0.762, perplexity 1.5142\n",
      "19 | train: loss 0.7754, acc 0.763, perplexity: 1.5575 | val: loss 0.7741, acc 0.764, perplexity 1.5092\n",
      "20 | train: loss 0.7690, acc 0.765, perplexity: 1.5492 | val: loss 0.7672, acc 0.766, perplexity 1.5063\n",
      "21 | train: loss 0.7616, acc 0.767, perplexity: 1.5439 | val: loss 0.7637, acc 0.767, perplexity 1.4992\n",
      "22 | train: loss 0.7545, acc 0.769, perplexity: 1.5384 | val: loss 0.7596, acc 0.767, perplexity 1.4947\n",
      "23 | train: loss 0.7475, acc 0.770, perplexity: 1.5319 | val: loss 0.7556, acc 0.768, perplexity 1.4890\n",
      "24 | train: loss 0.7419, acc 0.772, perplexity: 1.5257 | val: loss 0.7505, acc 0.770, perplexity 1.4866\n",
      "25 | train: loss 0.7347, acc 0.774, perplexity: 1.5198 | val: loss 0.7471, acc 0.770, perplexity 1.4808\n",
      "26 | train: loss 0.7283, acc 0.776, perplexity: 1.5134 | val: loss 0.7446, acc 0.771, perplexity 1.4765\n",
      "27 | train: loss 0.7217, acc 0.777, perplexity: 1.5089 | val: loss 0.7402, acc 0.772, perplexity 1.4725\n",
      "28 | train: loss 0.7161, acc 0.779, perplexity: 1.5028 | val: loss 0.7368, acc 0.773, perplexity 1.4694\n",
      "29 | train: loss 0.7106, acc 0.780, perplexity: 1.4973 | val: loss 0.7349, acc 0.773, perplexity 1.4633\n",
      "30 | train: loss 0.7044, acc 0.782, perplexity: 1.4915 | val: loss 0.7319, acc 0.775, perplexity 1.4589\n",
      "31 | train: loss 0.6989, acc 0.784, perplexity: 1.4866 | val: loss 0.7293, acc 0.776, perplexity 1.4587\n",
      "32 | train: loss 0.6934, acc 0.785, perplexity: 1.4819 | val: loss 0.7260, acc 0.777, perplexity 1.4571\n",
      "33 | train: loss 0.6886, acc 0.787, perplexity: 1.4778 | val: loss 0.7236, acc 0.778, perplexity 1.4513\n",
      "34 | train: loss 0.6828, acc 0.789, perplexity: 1.4739 | val: loss 0.7204, acc 0.778, perplexity 1.4494\n",
      "35 | train: loss 0.6781, acc 0.789, perplexity: 1.4677 | val: loss 0.7175, acc 0.780, perplexity 1.4469\n",
      "36 | train: loss 0.6728, acc 0.791, perplexity: 1.4639 | val: loss 0.7153, acc 0.781, perplexity 1.4423\n",
      "37 | train: loss 0.6679, acc 0.792, perplexity: 1.4598 | val: loss 0.7129, acc 0.782, perplexity 1.4410\n",
      "38 | train: loss 0.6620, acc 0.794, perplexity: 1.4553 | val: loss 0.7118, acc 0.783, perplexity 1.4354\n",
      "39 | train: loss 0.6589, acc 0.795, perplexity: 1.4508 | val: loss 0.7102, acc 0.784, perplexity 1.4317\n",
      "40 | train: loss 0.6540, acc 0.796, perplexity: 1.4470 | val: loss 0.7081, acc 0.784, perplexity 1.4306\n",
      "41 | train: loss 0.6492, acc 0.798, perplexity: 1.4430 | val: loss 0.7055, acc 0.786, perplexity 1.4285\n",
      "42 | train: loss 0.6447, acc 0.799, perplexity: 1.4404 | val: loss 0.7034, acc 0.785, perplexity 1.4259\n",
      "43 | train: loss 0.6405, acc 0.800, perplexity: 1.4364 | val: loss 0.7026, acc 0.787, perplexity 1.4229\n",
      "44 | train: loss 0.6367, acc 0.801, perplexity: 1.4321 | val: loss 0.7020, acc 0.787, perplexity 1.4217\n",
      "45 | train: loss 0.6335, acc 0.803, perplexity: 1.4295 | val: loss 0.6998, acc 0.788, perplexity 1.4204\n",
      "46 | train: loss 0.6279, acc 0.804, perplexity: 1.4261 | val: loss 0.6982, acc 0.788, perplexity 1.4194\n",
      "47 | train: loss 0.6239, acc 0.805, perplexity: 1.4221 | val: loss 0.6985, acc 0.789, perplexity 1.4139\n",
      "48 | train: loss 0.6209, acc 0.806, perplexity: 1.4190 | val: loss 0.6970, acc 0.789, perplexity 1.4117\n",
      "49 | train: loss 0.6160, acc 0.807, perplexity: 1.4163 | val: loss 0.6959, acc 0.790, perplexity 1.4118\n",
      "50 | train: loss 0.6137, acc 0.808, perplexity: 1.4130 | val: loss 0.6951, acc 0.791, perplexity 1.4077\n",
      "51 | train: loss 0.6110, acc 0.809, perplexity: 1.4097 | val: loss 0.6960, acc 0.791, perplexity 1.4080\n",
      "52 | train: loss 0.6071, acc 0.810, perplexity: 1.4077 | val: loss 0.6955, acc 0.791, perplexity 1.4068\n",
      "53 | train: loss 0.6039, acc 0.811, perplexity: 1.4043 | val: loss 0.6946, acc 0.792, perplexity 1.4061\n",
      "54 | train: loss 0.6003, acc 0.812, perplexity: 1.4024 | val: loss 0.6945, acc 0.793, perplexity 1.4041\n",
      "55 | train: loss 0.5974, acc 0.813, perplexity: 1.3995 | val: loss 0.6936, acc 0.792, perplexity 1.3992\n",
      "56 | train: loss 0.5942, acc 0.814, perplexity: 1.3966 | val: loss 0.6939, acc 0.793, perplexity 1.4001\n",
      "57 | train: loss 0.5912, acc 0.814, perplexity: 1.3941 | val: loss 0.6937, acc 0.792, perplexity 1.3976\n",
      "58 | train: loss 0.5882, acc 0.816, perplexity: 1.3921 | val: loss 0.6936, acc 0.793, perplexity 1.3938\n",
      "59 | train: loss 0.5860, acc 0.816, perplexity: 1.3899 | val: loss 0.6927, acc 0.794, perplexity 1.3952\n",
      "60 | train: loss 0.5817, acc 0.818, perplexity: 1.3871 | val: loss 0.6931, acc 0.793, perplexity 1.3911\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "N_HIDDEN = 256\n",
    "EMB_DIM = 200\n",
    "EPOCHS = 60\n",
    "N_LAYERS = 2\n",
    "\n",
    "model = CharRnnModel(N_HIDDEN, EMB_DIM, len(char_voc), num_layers=N_LAYERS)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-5)\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "train_loader = DataLoader(train_dataset, BATCH_SIZE, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, BATCH_SIZE, num_workers=4)\n",
    "\n",
    "model.to(device)\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    train_loss, train_acc, train_perplexity = train(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc, val_perplexity = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print('{} | train: loss {:.4f}, acc {:.3f}, perplexity: {:.4f} | val: loss {:.4f}, acc {:.3f}, perplexity {:.4f}'\n",
    "          .format(epoch + 1, train_loss, train_acc, train_perplexity, val_loss, val_acc, val_perplexity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get probabilities for symbols in each word from validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "м(0.04), а(0.00), л(0.00), ь(0.00), т(0.03), и(0.04), й(0.02), с(0.01), к(0.00), и(0.01), й(0.00), \n",
      "р(0.04), а(0.05), с(0.21), ч(0.03), л(0.03), е(0.00), н(0.01), и(0.06), т(0.06), ь(0.00), \n",
      "л(0.03), о(0.16), п(0.00), а(0.01), т(0.01), ь(0.00), с(0.00), я(0.01), \n",
      "и(0.03), н(0.02), д(0.00), е(0.00), к(0.01), с(0.00), и(0.01), р(0.00), о(0.00), в(0.00), а(0.10), т(0.01), ь(0.00), \n",
      "с(0.10), в(0.02), о(0.00), е(0.18), в(0.00), р(0.00), е(0.00), м(0.00), е(0.00), н(0.00), н(0.00), ы(0.00), й(0.00), \n",
      "р(0.04), а(0.00), с(0.02), п(0.00), р(0.01), а(0.00), в(0.00), и(0.01), т(0.00), ь(0.00), \n",
      "з(0.04), а(0.10), т(0.00), о(0.00), р(0.01), о(0.00), п(0.00), и(0.00), т(0.00), ь(0.00), с(0.01), я(0.00), \n",
      "р(0.04), а(0.10), с(0.00), с(0.20), в(0.03), е(0.00), т(0.00), \n",
      "к(0.07), р(0.00), а(0.00), с(0.00), и(0.00), т(0.10), ь(0.19), \n",
      "п(0.12), е(0.11), р(0.00), е(0.00), у(0.29), с(0.12), т(0.00), р(0.01), о(0.64), и(0.00), т(0.00), ь(0.00), \n",
      "а(0.04), т(0.18), \n",
      "р(0.04), а(0.14), с(0.00), ч(0.01), л(0.00), е(0.01), н(0.00), я(0.00), т(0.38), ь(0.00), \n",
      "а(0.04), э(0.00), р(0.00), о(0.04), л(0.00), о(0.00), г(0.00), и(0.03), я(0.00), \n",
      "з(0.04), а(0.14), к(0.00), о(0.00), в(0.00), ы(0.00), в(0.00), а(0.86), т(0.38), ь(0.00), \n",
      "о(0.06), б(0.00), и(0.57), д(0.04), а(0.02), \n",
      "с(0.10), п(0.00), р(0.01), а(0.00), в(0.00), е(0.00), д(0.00), л(0.00), и(0.03), в(0.00), ы(0.00), й(0.00), \n",
      "з(0.04), н(0.21), о(0.09), й(0.00), \n",
      "с(0.10), у(0.04), м(0.00), а(0.00), с(0.00), ш(0.00), е(0.40), д(0.02), ш(0.00), и(0.00), й(0.00), \n",
      "п(0.12), р(0.27), о(0.00), м(0.00), е(0.01), н(0.00), и(0.11), в(0.00), а(0.00), т(0.01), ь(0.00), \n",
      "д(0.04), и(0.04), а(0.04), г(0.07), н(0.01), о(0.03), с(0.01), т(0.01), \n",
      "р(0.04), а(0.28), с(0.11), п(0.00), у(0.01), т(0.05), н(0.01), ы(0.00), й(0.01), \n",
      "в(0.07), о(0.30), д(0.00), о(0.00), н(0.25), а(0.07), п(0.00), о(0.00), р(0.01), н(0.01), ы(0.00), й(0.00), \n",
      "н(0.05), е(0.11), б(0.00), р(0.02), е(0.16), ж(0.01), н(0.12), о(0.03), с(0.02), т(0.08), ь(0.00), \n",
      "м(0.04), л(0.02), е(0.01), к(0.00), о(0.00), п(0.04), и(0.05), т(0.02), а(0.00), ю(0.00), щ(0.00), е(0.01), е(0.00), \n",
      "е(0.00), д(0.04), к(0.12), и(0.09), й(0.00), \n",
      "о(0.06), т(0.18), к(0.01), л(0.01), о(0.47), н(0.29), и(0.18), т(0.03), ь(0.00), \n",
      "п(0.12), о(0.12), м(0.01), е(0.00), щ(0.01), и(0.00), к(0.10), \n",
      "п(0.12), а(0.00), л(0.05), л(0.01), и(0.32), а(0.00), т(0.00), и(0.00), в(0.00), н(0.00), ы(0.00), й(0.00), \n",
      "п(0.12), е(0.19), р(0.02), е(0.01), к(0.00), а(0.00), т(0.75), ы(0.00), в(0.01), а(0.03), т(0.01), ь(0.97), с(0.00), я(0.00), \n",
      "у(0.04), л(0.15), е(0.00), ч(0.00), ь(0.00), с(0.00), я(0.00), \n",
      "п(0.12), о(0.37), к(0.01), р(0.26), у(0.10), т(0.00), и(0.02), т(0.13), ь(0.01), \n",
      "н(0.05), е(0.19), ч(0.01), ё(0.00), т(0.00), к(0.00), и(0.71), й(0.00), \n",
      "у(0.04), м(0.00), е(0.85), н(0.01), ь(0.00), ш(0.00), и(0.00), т(0.00), е(0.00), л(0.00), ь(0.00), н(0.00), о(0.00), \n",
      "в(0.07), к(0.00), а(0.00), п(0.00), ы(0.00), в(0.00), а(0.00), т(0.03), ь(0.00), \n",
      "с(0.10), н(0.01), а(0.00), й(0.00), п(0.00), е(0.00), р(0.00), с(0.00), к(0.26), и(0.01), й(0.00), \n",
      "р(0.04), ы(0.02), б(0.21), о(0.09), в(0.02), о(0.29), д(0.04), н(0.02), ы(0.01), й(0.00), \n",
      "к(0.07), а(0.26), б(0.00), е(0.00), с(0.11), т(0.11), а(0.97), н(0.00), \n",
      "в(0.07), б(0.01), е(0.01), г(0.00), а(0.03), т(0.39), ь(0.43), \n",
      "в(0.07), о(0.00), л(0.00), о(0.01), с(0.19), н(0.06), я(0.00), \n",
      "п(0.12), р(0.27), и(0.57), о(0.04), с(0.00), т(0.05), а(0.03), н(0.04), о(0.00), в(0.00), и(0.00), т(0.00), ь(0.00), \n",
      "о(0.06), ч(0.00), н(0.01), у(0.11), т(0.00), ь(0.00), с(0.02), я(0.00), \n",
      "к(0.07), о(0.00), н(0.00), к(0.00), у(0.00), р(0.00), е(0.01), н(0.00), ц(0.00), и(0.00), я(0.00), \n",
      "л(0.03), о(0.30), б(0.08), н(0.01), ы(0.00), й(0.00), \n",
      "о(0.06), в(0.00), ч(0.00), а(0.23), р(0.02), н(0.00), я(0.01), \n",
      "о(0.06), т(0.00), в(0.04), е(0.06), т(0.00), с(0.10), т(0.05), в(0.00), е(0.01), н(0.00), н(0.00), о(0.00), с(0.00), т(0.00), ь(0.00), \n",
      "ч(0.01), е(0.11), р(0.00), т(0.12), я(0.11), г(0.02), а(0.03), \n",
      "с(0.10), и(0.04), м(0.00), п(0.00), а(0.00), т(0.00), и(0.00), з(0.00), и(0.00), р(0.00), о(0.00), в(0.00), а(0.00), т(0.00), ь(0.00), \n",
      "х(0.01), м(0.00), ы(0.04), к(0.12), а(0.10), т(0.11), ь(0.03), \n",
      "п(0.12), р(0.01), о(0.03), п(0.01), а(0.01), ж(0.00), а(0.00), \n",
      "с(0.10), е(0.11), р(0.00), б(0.03), с(0.00), к(0.00), о(0.80), \n"
     ]
    }
   ],
   "source": [
    "loader = DataLoader(test_dataset, 1)\n",
    "model.eval()\n",
    "symbol_probs = []\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred, _ = model(x)\n",
    "        probs = torch.softmax(pred, dim=2)\n",
    "    \n",
    "    word = test_dataset.get_word(i)\n",
    "    symbol_probs.append([(word[j], probs[0][j][test_dataset.char_vocab.s2i(word[j])].item()) for j in range(len(word))])\n",
    "\n",
    "for word in symbol_probs[:50]:\n",
    "    for symbol in word:\n",
    "        print('{}({:.2f}), '.format(symbol[0], symbol[1]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_params': {\n",
    "        'n_hidden': N_HIDDEN,\n",
    "        'emb_dims': EMB_DIM,\n",
    "        'n_layers': N_LAYERS,\n",
    "        'dropout': 0.5\n",
    "    },\n",
    "    'vocab_info': {\n",
    "        'vocab': char_voc._symbols,\n",
    "        's2i': char_voc._s2i,\n",
    "        'i2s': {i:s for s, i in char_voc._s2i.items()},\n",
    "        'spec_tokens': AUXILIARY,\n",
    "    }\n",
    "}, 'charRNNmodel.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRnnModel(\n",
       "  (embed): Embedding(37, 200)\n",
       "  (gru): GRU(200, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=37, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
