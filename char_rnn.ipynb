{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from re import sub\n",
    "import string\n",
    "\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character level RNN for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to make a net which generates posts in style of the vk public page Чё (https://vk.com/21jqofa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main ideas were taken from udacity/deep-learning-v2-pytorch course. The link to the notebook https://github.com/udacity/deep-learning-v2-pytorch/blob/master/recurrent-neural-networks/char-rnn/Character_Level_RNN_Solution.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Posts from Чё were grabbed with API of VK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>from_id</th>\n",
       "      <th>to_id</th>\n",
       "      <th>date</th>\n",
       "      <th>marked_as_ads</th>\n",
       "      <th>post_type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4290712</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>1548585660</td>\n",
       "      <td>0</td>\n",
       "      <td>post</td>\n",
       "      <td>решил выделить время на учёбу.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4297228</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>1548680809</td>\n",
       "      <td>0</td>\n",
       "      <td>post</td>\n",
       "      <td>заходят как-то в бар ъеъ, ьеь и ъяь, а бармен ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4297040</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>1548677100</td>\n",
       "      <td>1</td>\n",
       "      <td>post</td>\n",
       "      <td>НЕ УСПЕЛИ ПОСТУПИТЬ В ВУЗ?&lt;br&gt;&lt;br&gt;В нашем инст...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4297014</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>1548676836</td>\n",
       "      <td>0</td>\n",
       "      <td>post</td>\n",
       "      <td>мы встроили тебе кота в кота, чтобы ты мог гла...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4296875</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>-53845179</td>\n",
       "      <td>1548675032</td>\n",
       "      <td>0</td>\n",
       "      <td>post</td>\n",
       "      <td>ехали медведи на велосипеде,&lt;br&gt;а за ними ксюх...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id   from_id     to_id        date  marked_as_ads post_type  \\\n",
       "0  4290712 -53845179 -53845179  1548585660              0      post   \n",
       "1  4297228 -53845179 -53845179  1548680809              0      post   \n",
       "2  4297040 -53845179 -53845179  1548677100              1      post   \n",
       "3  4297014 -53845179 -53845179  1548676836              0      post   \n",
       "4  4296875 -53845179 -53845179  1548675032              0      post   \n",
       "\n",
       "                                                text  \n",
       "0                     решил выделить время на учёбу.  \n",
       "1  заходят как-то в бар ъеъ, ьеь и ъяь, а бармен ...  \n",
       "2  НЕ УСПЕЛИ ПОСТУПИТЬ В ВУЗ?<br><br>В нашем инст...  \n",
       "3  мы встроили тебе кота в кота, чтобы ты мог гла...  \n",
       "4  ехали медведи на велосипеде,<br>а за ними ксюх...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('che_posts.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "решил выделить время на учёбу.\n",
      "\n",
      "\n",
      "заходят как-то в бар ъеъ, ьеь и ъяь, а бармен им и говорит:\n",
      "\n",
      "\n",
      "мы встроили тебе кота в кота, чтобы ты мог гладить кота, пока гладишь кота.\n",
      "\n",
      "\n",
      "ехали медведи на велосипеде,\n",
      "а за ними ксюха ехала кукухой.\n",
      "\n",
      "\n",
      "забирай себе, если ты один из:\n",
      "1. никто.\n",
      "2. ничто.\n",
      "3. не существуешь.\n",
      "4. тебя нет.\n",
      "5. никита.\n",
      "никто не узнает, кто ты именно, потому что всем плевать на тебя.\n",
      "\n",
      "\n",
      "стадии эволюции насти: \n",
      "1. настя. \n",
      "2. настюха. \n",
      "3. анастасия.\n",
      "4. ананастасия. \n",
      "5. настойка боярышника.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df[df['marked_as_ads'] != 1]\n",
    "df['text'] = df['text'].apply(lambda x: str(x))\n",
    "texts = df['text'].tolist()\n",
    "text = '\\n\\n\\n'.join(texts)\n",
    "text = sub('[<br>]+', '\\n', text)\n",
    "text = sub(r'[^а-яА-Яё\\d\\s{}]'.format(string.punctuation), '', text)\n",
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created class for handling text data by implementation of PyTorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, text, sequence_length):\n",
    "        self.text = text\n",
    "        self.vocab = set(self.text)\n",
    "        self.char2int = {c: i for i, c in enumerate(self.vocab)}\n",
    "        self.int2char = {i: c for c, i in self.char2int.items()}\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (len(self.text) - self.sequence_length - 1) // self.sequence_length\n",
    "    \n",
    "    def __getitem__(self, indx):\n",
    "        indx = indx * self.sequence_length\n",
    "        x = self._to_one_hot(text[indx: indx + self.sequence_length], self.char2int)\n",
    "        x = torch.from_numpy(x)\n",
    "        y = [self.char2int[c] for c in text[indx + 1: indx + self.sequence_length + 1]]\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return (x, y)\n",
    "        \n",
    "    def _to_one_hot(self, string, char2int):\n",
    "        one_hot = np.zeros((len(string), len(self.vocab)), dtype=np.float32)\n",
    "        for i, c in enumerate(string):\n",
    "            one_hot[i, char2int[c]] = 1.\n",
    "        return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRAIN = int(len(text) * 0.9)\n",
    "BATCH_SIZE = 128\n",
    "SEQ_LEN = 200\n",
    "\n",
    "train_dataset = TextDataset(text, SEQ_LEN)\n",
    "val_dataset = TextDataset(text, SEQ_LEN)\n",
    "assert len(train_dataset.vocab) == len(val_dataset.vocab)\n",
    "\n",
    "train_dataset.text = text[:NUM_TRAIN]\n",
    "val_dataset.text = text[NUM_TRAIN:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in train_loader:\n",
    "#     print(x.size(), y.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size,\n",
    "                 seq_len, lstm_layers, drop_rate):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm_layers = lstm_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers=lstm_layers,\n",
    "                           dropout=drop_rate, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_rate)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        lstm_out, hidden = self.lstm(inputs, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_size)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return (torch.zeros(self.lstm_layers, batch_size, self.hidden_size, dtype=torch.float32),\n",
    "                torch.zeros(self.lstm_layers, batch_size, self.hidden_size, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that model works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(110, 128, num_layers=2, batch_first=True)\n",
      "  (dropout): Dropout(p=0)\n",
      "  (fc): Linear(in_features=128, out_features=110, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_params = {\n",
    "    'input_size': len(train_dataset.vocab),\n",
    "    'hidden_size': 128,\n",
    "    'output_size': len(train_dataset.vocab),\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'lstm_layers': 2,\n",
    "    'drop_rate': 0,\n",
    "}\n",
    "model = CharRNN(**model_params)\n",
    "print(model)\n",
    "h = model.init_hidden(BATCH_SIZE)\n",
    "\n",
    "for x, y in train_loader:\n",
    "    model(x, h)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_val_accuracy(model, val_loader, criterion):\n",
    "    losses = []\n",
    "    h = model.init_hidden(batch_size=val_loader.batch_size)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            h = tuple([i.data for i in h])\n",
    "            h = (h[0].to(device), h[1].to(device))\n",
    "            \n",
    "            if x.size()[0] != val_loader.batch_size:\n",
    "                continue\n",
    "            \n",
    "            pred, h = model(x, h)\n",
    "            \n",
    "            loss = criterion(pred, y.view(-1))\n",
    "            losses.append(loss.item())\n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, epochs, criterion, optimizer, val_loader=None, reduce_sch=None):\n",
    "    model.train()\n",
    "    h = model.init_hidden(batch_size=train_loader.batch_size)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_losses = []\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            if x.size()[0] != train_loader.batch_size:\n",
    "                continue\n",
    "            h = ([i.data.to(device) for i in h])\n",
    "\n",
    "            model.zero_grad()\n",
    "            pred, h = model(x, h)\n",
    "            loss = criterion(pred, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        if val_loader:\n",
    "            avg_val_loss = check_val_accuracy(model, val_loader, criterion)\n",
    "            if reduce_sch:\n",
    "                reduce_sch.step(avg_val_loss)\n",
    "            print('Epoch {}, train loss {:.5f}, val loss {:.5f}'.format(\n",
    "                    epoch + 1, avg_train_loss, avg_val_loss))\n",
    "        else:\n",
    "            print('Epoch {}, train loss {:.5f}'.format(\n",
    "                    epoch + 1, avg_train_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    'input_size': len(train_dataset.vocab),\n",
    "    'hidden_size': 256,\n",
    "    'output_size': len(train_dataset.vocab),\n",
    "    'seq_len': SEQ_LEN,\n",
    "    'lstm_layers': 2,\n",
    "    'drop_rate': 0.3,\n",
    "}\n",
    "epochs = 60\n",
    "\n",
    "model = CharRNN(**model_params)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = ReduceLROnPlateau(optim, factor=0.5, patience=5, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train loss 3.39923, val loss 3.37938\n",
      "Epoch 2, train loss 3.11769, val loss 2.96956\n",
      "Epoch 3, train loss 2.80821, val loss 2.82081\n",
      "Epoch 4, train loss 2.69381, val loss 2.69505\n",
      "Epoch 5, train loss 2.59977, val loss 2.61458\n",
      "Epoch 6, train loss 2.53313, val loss 2.55107\n",
      "Epoch 7, train loss 2.47714, val loss 2.49522\n",
      "Epoch 8, train loss 2.42682, val loss 2.44397\n",
      "Epoch 9, train loss 2.38073, val loss 2.39907\n",
      "Epoch 10, train loss 2.34166, val loss 2.35679\n",
      "Epoch 11, train loss 2.29977, val loss 2.31792\n",
      "Epoch 12, train loss 2.26048, val loss 2.27670\n",
      "Epoch 13, train loss 2.22278, val loss 2.23638\n",
      "Epoch 14, train loss 2.18552, val loss 2.20160\n",
      "Epoch 15, train loss 2.15287, val loss 2.17668\n",
      "Epoch 16, train loss 2.13412, val loss 2.14477\n",
      "Epoch 17, train loss 2.09595, val loss 2.11012\n",
      "Epoch 18, train loss 2.06715, val loss 2.08304\n",
      "Epoch 19, train loss 2.04201, val loss 2.06168\n",
      "Epoch 20, train loss 2.01785, val loss 2.03632\n",
      "Epoch 21, train loss 1.99475, val loss 2.01444\n",
      "Epoch 22, train loss 1.97392, val loss 1.99480\n",
      "Epoch 23, train loss 1.95347, val loss 1.97479\n",
      "Epoch 24, train loss 1.93438, val loss 1.95462\n",
      "Epoch 25, train loss 1.91686, val loss 1.93908\n",
      "Epoch 26, train loss 1.89993, val loss 1.92114\n",
      "Epoch 27, train loss 1.88424, val loss 1.90495\n",
      "Epoch 28, train loss 1.86875, val loss 1.88976\n",
      "Epoch 29, train loss 1.85438, val loss 1.87635\n",
      "Epoch 30, train loss 1.84187, val loss 1.86540\n",
      "Epoch 31, train loss 1.82881, val loss 1.85128\n",
      "Epoch 32, train loss 1.81650, val loss 1.83807\n",
      "Epoch 33, train loss 1.80555, val loss 1.82856\n",
      "Epoch 34, train loss 1.79453, val loss 1.81879\n",
      "Epoch 35, train loss 1.78445, val loss 1.80666\n",
      "Epoch 36, train loss 1.77458, val loss 1.79981\n",
      "Epoch 37, train loss 1.76568, val loss 1.78718\n",
      "Epoch 38, train loss 1.75703, val loss 1.77931\n",
      "Epoch 39, train loss 1.74830, val loss 1.77122\n",
      "Epoch 40, train loss 1.74011, val loss 1.76436\n",
      "Epoch 41, train loss 1.73279, val loss 1.75536\n",
      "Epoch 42, train loss 1.72544, val loss 1.74921\n",
      "Epoch 43, train loss 1.71926, val loss 1.74283\n",
      "Epoch 44, train loss 1.71220, val loss 1.73604\n",
      "Epoch 45, train loss 1.70616, val loss 1.72985\n",
      "Epoch 46, train loss 1.69950, val loss 1.72610\n",
      "Epoch 47, train loss 1.69923, val loss 1.72153\n",
      "Epoch 48, train loss 1.69063, val loss 1.71615\n",
      "Epoch 49, train loss 1.68429, val loss 1.70790\n",
      "Epoch 50, train loss 1.67839, val loss 1.70449\n",
      "Epoch 51, train loss 1.67384, val loss 1.69977\n",
      "Epoch 52, train loss 1.66812, val loss 1.69324\n",
      "Epoch 53, train loss 1.66394, val loss 1.68976\n",
      "Epoch 54, train loss 1.65916, val loss 1.68747\n",
      "Epoch 55, train loss 1.65531, val loss 1.68159\n",
      "Epoch 56, train loss 1.65106, val loss 1.67586\n",
      "Epoch 57, train loss 1.66657, val loss 1.69212\n",
      "Epoch 58, train loss 1.66631, val loss 1.68935\n",
      "Epoch 59, train loss 1.65557, val loss 1.67744\n",
      "Epoch 60, train loss 1.64684, val loss 1.66907\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, epochs, criterion, optim, val_loader, scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'char_rnn_сру_60_epochs.tar'\n",
    "\n",
    "torch.save({\n",
    "    'epoch': 60,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_params': model_params,\n",
    "    'optimizer_state_dict': optim.state_dict()\n",
    "}, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict one symbol from the current character and hidden state. Next char being got randomly from the top_k most probable symbols according to their probabilities (top_k simbols have different probabilities to be chosen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, char, h, top_k):\n",
    "    x = train_dataset._to_one_hot(char, train_dataset.char2int)\n",
    "    x = torch.from_numpy(np.array([x])).to(device)\n",
    "    h = ([i.data.to(device) for i in h])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pred, h = model(x, h)\n",
    "        probs = F.softmax(pred, dim=1).data\n",
    "        probs.cpu()\n",
    "        top = probs.topk(top_k)\n",
    "        \n",
    "        probs = top[0].cpu().detach().numpy().squeeze()\n",
    "        top_ch = top[1].cpu().numpy().squeeze()\n",
    "        next_char = np.random.choice(top_ch, p=probs/probs.sum())\n",
    "    \n",
    "    return train_dataset.int2char[next_char], h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict symbols one by one. The initial hidden state is computed from characters given from a prime string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, size, prime_string, top_k):\n",
    "    model.eval()\n",
    "    \n",
    "    chars = [i for i in prime_string]\n",
    "    \n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prime_string:\n",
    "        char, h = predict(model, ch, h, top_k)\n",
    "        \n",
    "    chars.append(char)\n",
    "    \n",
    "    for _ in range(size):\n",
    "        char, h = predict(model, chars[-1], h, top_k)\n",
    "        chars.append(char)\n",
    "    \n",
    "    print(''.join(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Одним зимнем холодным вечером великой дела. пока солнце, не мой выбраться в портуп, а продавило пароводии перед девчумстом, которые обратные подродение, но переломали немного весельные стина про свой на сексу, подошёл просил испалик, что я пришёлся в собственному свете не в подушке, а не смотрела на коренно пришли на карманах.\n",
      "\n",
      "\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером и весёлые мотоциобно в мире с костюм.\n",
      "\n",
      "\n",
      "восточные водолей, а подушки в меня и в совершенно облаки на машенки начинать своего сложности, а нужно пытаются выправить себя приделом и спецердательных картов с проблемой с красная и скресим.\n",
      "\n",
      "\n",
      "а спонсор этого дня  стал свои стольком и просимой свое встать.\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером и подарить своего меня на столовой прославной какую предлезую стоит в просности представляющей красотом на волосину проступатетей.\n",
      "\n",
      "\n",
      "а спонсор этого вечера  сосисочка видят невернулась.\n",
      "парень в подверном. сосудик с пакетов по кленату.\n",
      "\n",
      "\n",
      "первый сердце не подозречают стал и всегда вот в троме. тут ст\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером.\n",
      "\n",
      "\n",
      "просто составшимся, что там не вышет не проснулся в песердучей с клупа из пологеней, а то на каждый как-то пришил под под него станой в капола пастой своих стреля.\n",
      "4. пидор песни по крыльцом и просто вечно, чтобы обороны и придосять вам подушка, купило витали не вышла помощенных мир и ключато на \n",
      "------------------------------\n",
      "Одним зимнем холодным вечером по перестанка и страна. не подумали. я спал на минутор страшно не вылезать троголами парник на перестоп.\n",
      "\n",
      "\n",
      "всё подозряю, но не в ного ночи.\n",
      "\n",
      "\n",
      "помню наутнее, я вилял,\n",
      "а темпер и сегодня. так в тех, кто ты смешаров из сектами и всё такой возвращайся. попросил в стену свинье, на сосними неспокойно посм\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером. но выстрелите, что вы скрывают в компании и посмотрю в соле и предлетил паркет. нет с сами делает странное самые светлительных собрания в разрезать секса и выставалось по всё в призвате свет и настал в себя и кровью с тобой по клобку.\n",
      "\n",
      "\n",
      "в котором я выбирал какой-то ворством, а потом с кироленными с\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером.\n",
      "я: *водит, стоя на самом деле она: \"правда, просто строит, потому что ты дома, собака\", секунду на себе и пришёл на передолжительно осознала себя с состановкой. на которое стала в конце провесте свидетелем, что он на порожается, чтобы не высидить на коммунист на переводе в совет по страницу собралс\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером. \n",
      " \n",
      "1. стоять по клубе поможет выпала с пальмы. \n",
      "10. начала не понять.\n",
      "20, 14, 21, 28, 28  вообще выйдешь ваш воды и стали с пидором проводить все вообще. \n",
      "2. скажите, как она паратас вы спокинали в себя всё под крутает в месяце из-та ниместного спатья. \n",
      "2. поставиться на светле про самый костюм с т\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером.\n",
      "1. напросер прават вам с свой родом.\n",
      "3. ну и ты всю возьму на корей препрачу ножим.\n",
      "2. весь высокой сосед не спать, но и просто сосиски плачатировать, когда остальные водитель посли, кровать свои которых себя не слышите, чтобы вы встретить ваше вода, как тебе в пореди солна в себя.\n",
      "2. в подушку про\n",
      "------------------------------\n",
      "Одним зимнем холодным вечером, состоянность из которого с тобой нас составить свою половину с ними пришёл как с комбата с печалом\" \"не плевать на корменьи\".\n",
      "\n",
      "\n",
      " ты мой бил, ты? \n",
      " нет.\n",
      " просто. \n",
      " нас такой ванны, но вы спреминается, скажи, что ты спросишь, как подостольши, кто тебя, не в небе вы имеет? просто плохо из-за медель не\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    sample(model, 300, 'Одним зимнем холодным вечером', 5)\n",
    "    print('-'*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
